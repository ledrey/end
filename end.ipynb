{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2d83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('data.tsv', sep = '\\t', keep_default_na = False, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16da4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer\n",
    "\n",
    "def delSW(text):\n",
    "  stop_words = stopwords.words('english')\n",
    "  stop_words.extend([',','.',';',':','(',')',\"'\",'s'])\n",
    "  return [word for word in text if word not in stop_words]\n",
    "\n",
    "def lem(text):\n",
    "  wnl = WordNetLemmatizer()\n",
    "  return [wnl.lemmatize(w) for w in text]\n",
    "\n",
    "def stem(text):\n",
    "  array = []\n",
    "  for word in text:\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    word = stemmer.stem(word)\n",
    "    array.append(word)\n",
    "  return array\n",
    "\n",
    "nltk_data = data\n",
    "\n",
    "# nltk_data['tokened'] = nltk_data.abstract.str.lower().apply(word_tokenize).apply(delSW)\n",
    "# nltk_data['lemmed'] = nltk_data.tokened.apply(lem)\n",
    "# nltk_data['stemmed'] = nltk_data.tokened.apply(stem)\n",
    "\n",
    "nltk_data.to_csv('nltk_data_prepped.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155dc6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('firm', 59073),\n",
       " ('study', 35965),\n",
       " ('s', 35157),\n",
       " ('model', 34665),\n",
       " ('effect', 33078),\n",
       " ('result', 29513),\n",
       " ('research', 25971),\n",
       " ('market', 25653),\n",
       " ('find', 24762),\n",
       " ('performance', 24605)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(''.join(str(v).replace('\"','')\n",
    "  .replace(\"'\",'')\n",
    "  .replace('[','')\n",
    "  .replace(']','')\n",
    "  .replace(',','') for v in nltk_data.lemmed).split()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71ce2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_data = data\n",
    "gensim_data['abst'] = gensim_data.abstract.map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "gensim_data['abst'] = gensim_data.abst.map(lambda x: x.lower())\n",
    "gensim_data.abst.head()\n",
    "gensim_data.to_csv('gensim_data_prepped.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "325a91ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words_gensim = stopwords.words('english')\n",
    "stop_words_gensim.extend(['from', 'subject', 're', 'edu', 'use', 'research', 'study'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  return [[word for word in simple_preprocess(str(doc)) if word not in stop_words_gensim] for doc in text]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "abstracts_isolated = gensim_data.abst.apply(lambda x: \" \".join([y.lemma_ for y in nlp(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2df0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_isolated = pd.read_csv('lemmed.tsv', sep = '\\t').drop('Unnamed: 0', axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b8e9f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_isolated_edited = abstracts_isolated.values.tolist()\n",
    "abstracts_words_isolated = list(sent_to_words(abstracts_isolated_edited))\n",
    "abstracts_words_isolated = remove_stopwords(abstracts_isolated_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6ad7ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_isolated.to_csv('lemmed.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "259b5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = corpora.Dictionary(abstracts_words_isolated)\n",
    "corpus = [id2word.doc2bow(text) for text in abstracts_words_isolated]\n",
    "lda_model = gensim.models.LdaMulticore(corpus = corpus, id2word = id2word, num_topics = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cea046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyLDAvis import gensim_models as gensimvis\n",
    "from pyLDAvis import enable_notebook, save_html\n",
    "enable_notebook()\n",
    "\n",
    "model = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "save_html(model, 'viz.html')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0d46c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from countrygroups import EUROPEAN_UNION\n",
    "\n",
    "countries = EUROPEAN_UNION.names\n",
    "countries.extend(['England','Wales','Scotland'])\n",
    "type(countries)\n",
    "schools = data.addresses\n",
    "schools = schools.str.extractall(r'(?<=\\] )(.*?)(?=\\;|$)').melt().drop('variable', axis = 1)\n",
    "schools['university'] = schools.value.str.extract(r'^(.+?),')\n",
    "schools['country'] = schools.value.str.extract(r'^.*\\, (.*)$')\n",
    "schools = schools[schools.country.isin(countries)].drop_duplicates(subset = 'university')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a73b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_list = pd.read_csv('list_of_schools.csv', sep = ';').drop('#', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "865977d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method capitalize of str object at 0x104b2bf70>\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "for school in schools:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
