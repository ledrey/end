{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "6f2d83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('data.tsv', sep = '\\t', keep_default_na = False, low_memory=False)\n",
    "data_req = data[['article_title','authors','source_title','times_cited_all','times_cited_wos','180_days_usage','since_2013_usage','publication_year','number_of_pages','wos_categories','research_areas','highly_cited_status','hot_paper_status','funding_text']]\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee53a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session('notebook_env.db') # Save session to dump file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e81bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.load_session('notebook_env.db') # Restore sesion from dump file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec1bda",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b71ce2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Abstract preprocess\n",
    "\n",
    "gensim_data = data.abstract.map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "gensim_data = gensim_data.map(lambda x: x.lower())\n",
    "gensim_data.head()\n",
    "gensim_data.to_csv('gensim_data_prepped.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab28b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "abstracts_isolated = gensim_data.abst.apply(lambda x: \" \".join([y.lemma_ for y in nlp(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9715a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data preprocessing dataset, extract addresses and parties from publication metadata\n",
    "\n",
    "from countrygroups import EUROPEAN_UNION\n",
    "\n",
    "countries = EUROPEAN_UNION.names\n",
    "countries.extend(['England','Wales','Scotland','Switzerland','Norway']) # Extend non-EU countries to match continental Europe\n",
    "data_calc = data[['article_title','addresses']]\n",
    "data_calc = data_calc[data_calc.addresses != ''] # Filter out publications with no address submitted\n",
    "data_calc['addresses'] = data_calc['addresses'].str.replace(' *\\[[^\\]]*] ', '', regex=True) # Remove [Author/Authors]\n",
    "data_calc['addresses'] = data_calc['addresses'].str.split(';') # Create a list of addresses\n",
    "data_calc = data_calc.explode('addresses')\n",
    "# data_calc = data_calc['addresses'].str.extractall(r'(?<=\\] )(.*?)(?=\\;|$)|(?=^[^\\[])(.*?)(?=\\;|$)').droplevel(1).drop(labels=1, axis=1)\n",
    "data_calc['university'] = data_calc['addresses'].str.extract(r'^(.+?),')\n",
    "data_calc['country'] = data_calc['addresses'].str.extract(r'^.*\\, (.*)$')\n",
    "# data_calc = data_calc[data_calc.groupby(level=0)['country'].transform(lambda x : x.isin(countries).any())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "302d155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match extracted parties to FT-list European business schools\n",
    "\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "schools_ft = pd.read_csv('schools_ft.csv', header=None, sep='\\t')\n",
    "data_calc['bs']=data_calc['university'].apply(lambda x : [process.extractOne(x, schools_ft[0], score_cutoff=88)])\n",
    "\n",
    "def extract_school(data): # NaN value fix\n",
    "  res = []\n",
    "  for bs in data:\n",
    "    if bs[0] is not None:\n",
    "      res.append(bs[0][0])\n",
    "    else:\n",
    "      res.append('')\n",
    "\n",
    "  return res\n",
    "\n",
    "data_calc['bs'] = extract_school(data_calc['bs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "14106810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binaries true/false for each party of each publication\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_calc = data_calc.join(data[['funding_text']])\n",
    "\n",
    "def bs_tf(row): # Business school  \n",
    "  if row['bs'] == '':\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "def univ_tf(row): # University \n",
    "  if 'Univ' in row['university'] or 'univ' in row['university']:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "data_calc['bs_tf'] = data_calc.apply(bs_tf, axis=1) # Apply\n",
    "data_calc['univ_tf'] = data_calc.apply(univ_tf, axis=1) # Apply\n",
    "\n",
    "def univ_bs_tf(row): # Whether business school is university-based\n",
    "  if row['bs_tf'] and row['univ_tf']:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def fund_tf(row): # Funding text\n",
    "  if row['funding_text'] != '':\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "data_calc['univ_bs_tf'] = data_calc.apply(univ_bs_tf, axis=1) # Apply\n",
    "data_calc['fund_tf'] = data_calc.apply(fund_tf, axis=1) # Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion of parties data preparation\n",
    "\n",
    "data_fin = pd.DataFrame({'count' : data_calc.groupby(level=0).size(), # Total parties involved\n",
    "                    'bs_count': data_calc.groupby(level=0)['bs_tf'].sum(), # Total FT-list business schools\n",
    "                    'bs_bin': np.where(data_calc.groupby(level=0)['bs_tf'].sum() > 0, 1, 0), # T/F\n",
    "                    'univ_count': data_calc.groupby(level=0)['univ_tf'].sum(), # Total universities\n",
    "                    'univ_bin': np.where(data_calc.groupby(level=0)['univ_tf'].sum() > 0, 1, 0), # T/F\n",
    "                    'univ_bs_count': data_calc.groupby(level=0)['univ_bs_tf'].sum(), # Total university-based FT-list\n",
    "                    'univ_bs_bin': np.where(data_calc.groupby(level=0)['univ_bs_tf'].sum() > 0, 1, 0), # T/F\n",
    "                    'fund_bin': np.where(data_calc.groupby(level=0)['fund_tf'].sum() > 0, 1, 0)}) # T/F on funding (value transfer)\n",
    "data_fin['notuniv_bs_bin'] = np.where((data_fin.bs_bin == 1) & (data_fin.univ_bs_bin == 0), 1, 0) # Not university-based FT-list T/F\n",
    "\n",
    "data_fin = data_req.join(data_fin, how='right') # Final prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "896a7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save calculated/matched business schools to all authors for all papers\n",
    "\n",
    "data_calc.to_csv('data_calc_fuzzy.csv', sep = '\\t', na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "94b6a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved data\n",
    "\n",
    "data_calc = pd.read_csv('data_calc_fuzzy.csv', sep = '\\t')\n",
    "\n",
    "def del_none(data): # Function in case of [None]\n",
    "  res = []\n",
    "  for bs in data:\n",
    "    if bs != '[None]':\n",
    "      res.append(bs)\n",
    "    else:\n",
    "      res.append('')\n",
    "\n",
    "  return res\n",
    "\n",
    "data_calc['bs'] = del_none(data_calc['bs']) # Fix in case of [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "582d9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously lemmed abstracts\n",
    "\n",
    "abstracts_isolated = pd.read_csv('abstracts_lemmed.tsv', sep='\\t', index_col='Unnamed: 0')\n",
    "abstracts_isolated.to_csv('abstracts_lemmed.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "325a91ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare abstracts: tokenization and stop-word removal \n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "my_stop_words = STOPWORDS.union(set(['high','new','finding','theory','propose','article','set','focus','elsevi','consider','offer','present','suggest','test','impact','level','relationship','implication','support','suggest','analysis','subject','re','edu','use','research','study','result','firm','model','effect','problem','performance','datum','paper','information','examine','role','develop','increase','provide','relationship','author','affect','demonstrate','include','evidence','explore','literature','relate','base','investigate','manager','management','approach','strong','influence','practice','potential','framework','difference','argue','time','process','practice','low','empirical','issue','negative','measure','create','lead','sample','outcome','type','positive','discuss','associate','design','change','significant','period','factor','analyze','individual','organisation','organization','organisational','organizational','decision','right','elsevi','reserve','john','wiley','sons','shed','light','doi','jibs','highlight','importance','wide','range','international','journal','summary']))\n",
    "\n",
    "def rm_stop(text):\n",
    "  return [[word for word in simple_preprocess(str(doc)) if word not in my_stop_words] for doc in text]\n",
    "\n",
    "abstracts_isolated = rm_stop(abstracts_isolated.abst.str.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "17f9d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model dataset and apply content filters\n",
    "\n",
    "model_calc = data_fin.join(pd.DataFrame({'abst': abstracts_isolated}))\n",
    "model_calc = model_calc.join(data[['doi','article_title']], rsuffix='_data')\n",
    "\n",
    "model_calc = model_calc[~model_calc.abst.isin([['nan']])] # Empty abstracts removal\n",
    "doi_filter = ['10.1002/smj.499','10.1007/s10551-009-0354-z','10.1177/1042258719890991','10.1287/mnsc.2020.3636','10.25300/MISQ/2018/11914','10.1287/mnsc.2020.3587','10.1007/s10551-010-0523-0','10.1007/s10551-010-0497-y','10.1007/s10551-010-0663-2','10.1007/s10551-010-0697-5']\n",
    "model_calc = model_calc[~model_calc.doi.isin(doi_filter)] # Duplicates by DOI removal (hand-picked)\n",
    "title_filter = ['Political Cycles and Stock Returns', 'Firm Volatility in Granular Networks','Copyrights and Creativity: Evidence from Italian Opera in the Napoleonic Age','Productivity and Organization in Portuguese Firms','Occupational Licensing and Maternal Health: Evidence from Early Midwifery Laws','Equilibrium Labor Market Search and Health Insurance Reform',\"The Influence of Retail Management's Use of Social Power on Corporate Ethical Values, Employee Commitment, and Performance\",'You Support Diversity, But Are You Ethical? Examining the Interactive Effects of Diversity and Ethical Climate Perceptions on Turnover Intentions','Advertising Spending and Media Bias: Evidence from News Coverage of Car Safety Recalls']\n",
    "model_calc = model_calc[~model_calc.article_title.isin(title_filter)] # Duplicates by title removal (hand-picked)\n",
    "model_calc = model_calc.drop('article_title_data', axis = 1)\n",
    "\n",
    "model_calc['publication_year'].loc[model_calc.publication_year == 0] = 2022 # Make early-access a 2022 publication\n",
    "model_calc = model_calc[model_calc.publication_year >= 2005] # Year filtering >2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "aeaf65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract wos categories and turn into area binaries\n",
    "\n",
    "model_calc.research_areas = model_calc.research_areas.str.split('; ') # Break RA into lists\n",
    "model_calc.wos_categories = model_calc.wos_categories.str.split('; ') # Break WOS cat into lists\n",
    "\n",
    "def extract_wos_areas(data):\n",
    "  data['index'] = data.index\n",
    "  cat1 = data.explode('research_areas').pivot_table(index='index', columns='research_areas', aggfunc=\"size\", fill_value=0)\n",
    "  cat2 = data.explode('wos_categories').pivot_table(index='index', columns='wos_categories', aggfunc='size', fill_value=0)\n",
    "\n",
    "\n",
    "  areas = cat1.join(cat2, rsuffix='_2') # Compose WOS area dataset\n",
    "\n",
    "  areas = areas.join(pd.DataFrame({ # Compute 3/5 areas based on categories\n",
    "    'phys_sci': sum([areas['Mathematics'],areas['Mathematics, Interdisciplinary Applications'],areas['Statistics & Probability']]),\n",
    "    'soc_sci': sum([areas['Business & Economics'],areas['Mathematical Methods In Social Sciences'],areas['Psychology'],areas['Social Sciences - Other Topics'],areas['Business'],areas['Business, Finance'],areas['Economics'],areas['Ethics'],areas['Management'],areas['Psychology, Applied'],areas['Psychology, Social'],areas['Social Sciences, Interdisciplinary'],areas['Social Sciences, Mathematical Methods']]),\n",
    "    'tech': sum([areas['Computer Science'],areas['Engineering'],areas['Information Science & Library Science'],areas['Information Science & Library Science_2'],areas['Operations Research & Management Science'],areas['Operations Research & Management Science_2'],areas['Computer Science, Information Systems'],areas['Engineering, Manufacturing']])\n",
    "  }))\n",
    "  areas['phys_sci'], areas['soc_sci'], areas['tech'] = np.where(areas.phys_sci > 0, 1, 0), np.where(areas.soc_sci > 0, 1, 0), np.where(areas.tech > 0, 1, 0)\n",
    "  return areas\n",
    "\n",
    "model_calc = model_calc.join(extract_wos_areas(model_calc)).drop('index', axis=1) # Join with model dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "4c7c4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for transfer\n",
    "\n",
    "model_calc.to_csv('data_to_model.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "44e6c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = model_calc.groupby(['publication_year','soc_sci','phys_sci','tech']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "07a45154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize gensim model from publiactions finalized dataset\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import ldamodel, CoherenceModel, LdaMulticore, Phrases, phrases\n",
    "\n",
    "id2word = corpora.Dictionary(model_calc.abst)\n",
    "corpus = [id2word.doc2bow(text) for text in model_calc.abst]\n",
    "lda_model = ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=20, random_state=100, chunksize=len(corpus), per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud of Top N words in each topic\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "cols = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000']\n",
    "topics = lda_model.show_topics(20, 15, formatted=False)\n",
    "\n",
    "cloud = WordCloud(stopwords=my_stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=15,\n",
    "                  colormap='tab20',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "fig, axes = plt.subplots(4,5, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac472cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count and keyword importance for topics\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "data_flat = [w for w_list in model_calc.abst for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "def word_importance(data):\n",
    "    out = []\n",
    "    for i, topic in data:\n",
    "        for word, weight in topic:\n",
    "            out.append([word, i , weight, counter[word]])\n",
    "    return out\n",
    "\n",
    "df = pd.DataFrame(word_importance(topics), columns=['word', 'topic_id', 'importance', 'word_count'])\n",
    "fig, axes = plt.subplots(4, 5, figsize=(16,10), sharey=True, dpi=160)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.020); ax.set_ylim(0, 25000)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.xaxis.set_ticks(df.loc[df.topic_id==i, 'word'])\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=90, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "2dbbfb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a best matching topic for each publciation\n",
    "\n",
    "def topic_assignment(data, id2word, ldamodel):\n",
    "    top_topics = []\n",
    "    for d in data.abst:\n",
    "        bow = id2word.doc2bow(d)\n",
    "        t = ldamodel.get_document_topics(bow)  \n",
    "        top_topics.append(t[0][0])\n",
    "    return top_topics\n",
    "\n",
    "model_calc['topic'] = topic_assignment(model_calc.abst, id2word, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "4c7c4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as final\n",
    "\n",
    "model_calc.to_csv('data_to_model.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b7a2c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4fe15fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 model built\n",
      "5 model built\n",
      "6 model built\n",
      "7 model built\n",
      "8 model built\n",
      "9 model built\n",
      "10 model built\n",
      "11 model built\n",
      "12 model built\n",
      "13 model built\n"
     ]
    }
   ],
   "source": [
    "# Coherence analysis to achieve N perfect topics\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gensim.models import LdaModel, CoherenceModel, LdaMulticore\n",
    "\n",
    "num_topics = list(range(10,14)[1:])\n",
    "\n",
    "def multiple_models_init(topicnumberlist, corpus=corpus, id2word=id2word):\n",
    "  LDA_models = {}\n",
    "  for i in topicnumberlist:\n",
    "    print(i, end=' ')\n",
    "    LDA_models[i] = LdaModel(corpus=corpus, id2word=id2word, num_topics=i, eval_every=1, chunksize=len(corpus), passes=2, random_state=42)\n",
    "    print('model built')\n",
    "    shown_topics = LDA_models[i].show_topics(num_topics=i, \n",
    "                                             num_words=15,\n",
    "                                             formatted=False)\n",
    "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]\n",
    "\n",
    "  return LDA_models, LDA_topics\n",
    "\n",
    "LDA_models, LDA_topics = multiple_models_init(num_topics, corpus, id2word)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4a6af77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(topic_1, topic_2):\n",
    "    \"\"\"\n",
    "    Derives the Jaccard similarity of two topics\n",
    "\n",
    "    Jaccard similarity:\n",
    "    - A statistic used for comparing the similarity and diversity of sample sets\n",
    "    - J(A,B) = (A ∩ B)/(A ∪ B)\n",
    "    - Goal is low Jaccard scores for coverage of the diverse elements\n",
    "    \"\"\"\n",
    "    intersection = set(topic_1).intersection(set(topic_2))\n",
    "    union = set(topic_1).union(set(topic_2))\n",
    "                    \n",
    "    return float(len(intersection))/float(len(union))\n",
    "\n",
    "def lda_stability(data, topicnumberlist):\n",
    "    LDA_stability = {}\n",
    "    for i in range(0, len(topicnumberlist)-1):\n",
    "        jaccard_sims = []\n",
    "        for t1, topic1 in enumerate(data[topicnumberlist[i]]): # pylint: disable=unused-variable\n",
    "            sims = []\n",
    "            for t2, topic2 in enumerate(data[topicnumberlist[i+1]]): # pylint: disable=unused-variable\n",
    "                sims.append(jaccard_similarity(topic1, topic2))    \n",
    "            jaccard_sims.append(sims)    \n",
    "        LDA_stability[topicnumberlist[i]] = jaccard_sims\n",
    "    return LDA_stability\n",
    "                \n",
    "mean_stabilities = [np.array(lda_stability(LDA_models,num_topics)[i]).mean() for i in num_topics[:-1]]\n",
    "coherences = [CoherenceModel(model=LDA_models[i], texts=model_calc.abst, dictionary=id2word, coherence='c_v').get_coherence()\\\n",
    "              for i in num_topics[:-1]]\n",
    "\n",
    "def ideal_topics(coherence, ldastability, topicnumberlist):\n",
    "    \"\"\"\n",
    "    Derives optimum number of topics based on model coherences and its topics overlap\n",
    "    \"\"\"\n",
    "    coh_sta_diffs = [coherence[i] - ldastability[i] for i in range(0, len(topicnumberlist)-1)] # limit topic numbers to the number of keywords\n",
    "    coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == max(coh_sta_diffs)]\n",
    "    ideal_topic_num = topicnumberlist[coh_sta_max_idxs[0]] # choose less topics in case there's more than one max\n",
    "    return ideal_topic_num\n",
    "\n",
    "ideal_topic_num = ideal_topics(coherences, mean_stabilities, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=mean_stabilities, label='Average Topic Overlap')\n",
    "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
    "\n",
    "ax.axvline(x=ideal_topic_num, label='Ideal Number of Topics', color='black')\n",
    "ax.axvspan(xmin=ideal_topic_num - 1, xmax=ideal_topic_num + 1, alpha=0.5, facecolor='grey')\n",
    "\n",
    "ax.set_ylim([0, max(max(mean_stabilities), max(coherences)) + (0.10 * max(max(mean_stabilities), max(coherences)))])\n",
    "ax.set_xlim([3, num_topics[-1]])\n",
    "                \n",
    "ax.axes.set_title('Model Metrics per Number of Topics', fontsize=25)\n",
    "ax.set_ylabel('Metric Level', fontsize=20)\n",
    "ax.set_xlabel('Number of Topics', fontsize=20)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
