{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6f2d83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('data.tsv', sep = '\\t', keep_default_na = False, low_memory=False)\n",
    "data_req = data[['article_title','times_cited_all','times_cited_wos','180_days_usage','since_2013_usage','publication_year','number_of_pages','wos_categories','research_areas','highly_cited_status','hot_paper_status','funding_text']]\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16da4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer\n",
    "\n",
    "def delSW(text):\n",
    "  stop_words = stopwords.words('english')\n",
    "  stop_words.extend([',','.',';',':','(',')',\"'\",'s'])\n",
    "  return [word for word in text if word not in stop_words]\n",
    "\n",
    "def lem(text):\n",
    "  wnl = WordNetLemmatizer()\n",
    "  return [wnl.lemmatize(w) for w in text]\n",
    "\n",
    "def stem(text):\n",
    "  array = []\n",
    "  for word in text:\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    word = stemmer.stem(word)\n",
    "    array.append(word)\n",
    "  return array\n",
    "\n",
    "nltk_data = data\n",
    "\n",
    "# nltk_data['tokened'] = nltk_data.abstract.str.lower().apply(word_tokenize).apply(delSW)\n",
    "# nltk_data['lemmed'] = nltk_data.tokened.apply(lem)\n",
    "# nltk_data['stemmed'] = nltk_data.tokened.apply(stem)\n",
    "\n",
    "nltk_data.to_csv('nltk_data_prepped.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155dc6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('firm', 59073),\n",
       " ('study', 35965),\n",
       " ('s', 35157),\n",
       " ('model', 34665),\n",
       " ('effect', 33078),\n",
       " ('result', 29513),\n",
       " ('research', 25971),\n",
       " ('market', 25653),\n",
       " ('find', 24762),\n",
       " ('performance', 24605)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(''.join(str(v).replace('\"','')\n",
    "  .replace(\"'\",'')\n",
    "  .replace('[','')\n",
    "  .replace(']','')\n",
    "  .replace(',','') for v in nltk_data.lemmed).split()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71ce2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_data = data\n",
    "gensim_data['abst'] = gensim_data.abstract.map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "gensim_data['abst'] = gensim_data.abst.map(lambda x: x.lower())\n",
    "gensim_data.abst.head()\n",
    "gensim_data.to_csv('gensim_data_prepped.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "325a91ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words_gensim = stopwords.words('english')\n",
    "stop_words_gensim.extend(['from', 'subject', 're', 'edu', 'use', 'research', 'study'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  return [[word for word in simple_preprocess(str(doc)) if word not in stop_words_gensim] for doc in text]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "abstracts_isolated = gensim_data.abst.apply(lambda x: \" \".join([y.lemma_ for y in nlp(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a2df0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_isolated = pd.read_csv('abstracts_lemmed.tsv', sep = '\\t').drop('Unnamed: 0', axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b8e9f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_isolated_edited = abstracts_isolated.values.tolist()\n",
    "abstracts_words_isolated = list(sent_to_words(abstracts_isolated_edited))\n",
    "abstracts_words_isolated = remove_stopwords(abstracts_isolated_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6ad7ef47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abstracts_isolated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ledrey/Documents/GitHub/end/end.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ledrey/Documents/GitHub/end/end.ipynb#ch0000007?line=0'>1</a>\u001b[0m abstracts_isolated\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mabstracts_lemmed.tsv\u001b[39m\u001b[39m'\u001b[39m, sep \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'abstracts_isolated' is not defined"
     ]
    }
   ],
   "source": [
    "abstracts_isolated.to_csv('abstracts_lemmed.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "259b5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = corpora.Dictionary(abstracts_words_isolated)\n",
    "corpus = [id2word.doc2bow(text) for text in abstracts_words_isolated]\n",
    "lda_model = gensim.models.LdaMulticore(corpus = corpus, id2word = id2word, num_topics = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cea046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyLDAvis import gensim_models as gensimvis\n",
    "from pyLDAvis import enable_notebook, save_html\n",
    "enable_notebook()\n",
    "\n",
    "model = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "save_html(model, 'viz.html')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0d46c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from countrygroups import EUROPEAN_UNION\n",
    "\n",
    "countries = EUROPEAN_UNION.names\n",
    "countries.extend(['England','Wales','Scotland','Switzerland','Norway'])\n",
    "data_calc = data[['article_title','addresses']]\n",
    "data_calc = data_calc['addresses'].str.extractall(r'(?<=\\] )(.*?)(?=\\;|$)').droplevel(1)\n",
    "data_calc['university'] = data_calc[0].str.extract(r'^(.+?),')\n",
    "data_calc['country'] = data_calc[0].str.extract(r'^.*\\, (.*)$')\n",
    "data_calc = data_calc[data_calc.groupby(level=0)['country'].transform(lambda x : x.isin(countries).any())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "302d155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "data_calc['bs']=data_calc['university'].apply(lambda x : [process.extractOne(x, schools_ft[0], score_cutoff=88)])\n",
    "\n",
    "def extract_school(data):\n",
    "  res = []\n",
    "  for bs in data:\n",
    "    if bs[0] is not None:\n",
    "      res.append(bs[0][0])\n",
    "    else:\n",
    "      res.append(bs)\n",
    "\n",
    "  return res\n",
    "\n",
    "data_calc['bs'] = extract_school(data_calc['bs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "14106810",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_calc = data_calc.join(data_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "562c40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_calc.to_csv('data_calc_init.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "25f6e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs_tf(row):\n",
    "  if row['bs'] != [None]:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def univ_tf(row):\n",
    "  if 'Univ' in row['university'] or 'univ' in row['university']:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def frn_tf(row):\n",
    "  if 'USA' in row['country'] or 'U.S.' in row['country'] or 'Canada' in row['country'] or 'China' in row['country'] or 'Japan' in row['country'] or 'Australia' in row['country'] or 'Korea' in row['country'] or 'Singapore' in row['country'] or 'India' in row['country'] or 'Pakistan' in row['country']:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "data_calc['bs_tf'] = data_calc.apply(bs_tf, axis=1)\n",
    "data_calc['univ_tf'] = data_calc.apply(univ_tf, axis=1)\n",
    "data_calc['frn_tf'] = data_calc.apply(frn_tf, axis=1)\n",
    "\n",
    "def univ_bs_tf(row):\n",
    "  if row['bs_tf'] and row['univ_tf']:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "def fund_tf(row):\n",
    "  if row['funding_text'] != '':\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "data_calc['univ_bs_tf'] = data_calc.apply(univ_bs_tf, axis=1)\n",
    "data_calc['fund_tf'] = data_calc.apply(fund_tf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b47454d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_calc.to_csv('data_calc_stage1.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ed732e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_binary = data_calc[['article_title','funding_text','bs_tf','univ_tf','frn_tf','univ_bs_tf','fund_tf']]\n",
    "test = pd.DataFrame({'count' : data_binary.groupby(level=0).size(), \n",
    "                    'bs_count': data_binary.groupby(level=0)['bs_tf'].sum(),\n",
    "                    'bs_bin': np.where(data_binary.groupby(level=0)['bs_tf'].sum() > 0, 1, 0),\n",
    "                    'univ_count': data_binary.groupby(level=0)['univ_tf'].sum(),\n",
    "                    'univ_bs_count': data_binary.groupby(level=0)['univ_bs_tf'].sum(),\n",
    "                    'fund_bin': np.where(data_binary.groupby(level=0)['fund_tf'].sum() > 0, 1, 0),\n",
    "                    'frn_bin': np.where(data_binary.groupby(level=0)['frn_tf'].sum() > 0, 1, 0)})\n",
    "\n",
    "data_preregr = data_req.join(test, how='right').join(abstracts_isolated)\n",
    "data_preregr.to_csv('data_preregr.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f549f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def extract_unique_schools(strings):\n",
    "  uniques = []\n",
    "\n",
    "  for string in strings:\n",
    "    if not uniques:\n",
    "      uniques.append(string)\n",
    "\n",
    "    for unique in uniques:\n",
    "      if fuzz.partial_ratio(unique.lower().strip(), string.lower().strip()) > 90:\n",
    "        break\n",
    "    else:\n",
    "        uniques.append(string)\n",
    "\n",
    "  return uniques\n",
    "\n",
    "schools_unique = extract_unique_schools(schools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5c1fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_unique = pd.DataFrame(schools_unique).sort_values(0).reset_index(drop=True)\n",
    "schools_unique.to_csv('schools_unique.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b07f2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_ft = pd.read_csv('schools_ft.csv', sep = '\\t', header = None)\n",
    "schools_unique = pd.read_csv('schools_unique.csv', sep = '\\t', index_col = None, names = ['university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_data = pd.read_csv('data_fuzzy_prepped.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "0124263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "regr_data['bs']=regr_data['university'].apply(lambda x : [process.extractOne(x, schools_ft[0], score_cutoff=88)])\n",
    "\n",
    "def extract_school(data):\n",
    "  res = []\n",
    "  for bs in data:\n",
    "    if bs[0] is not None:\n",
    "      res.append(bs[0][0])\n",
    "    else:\n",
    "      res.append(bs)\n",
    "\n",
    "  return res\n",
    "\n",
    "regr_data['bs'] = extract_school(regr_data['bs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "9dd0dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_data.to_csv('data_fuzzy_prepped.tsv', sep = '\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
